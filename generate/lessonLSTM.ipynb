{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "AJczXFtcnYug",
    "ExecuteTime": {
     "end_time": "2025-11-06T12:49:49.231440Z",
     "start_time": "2025-11-06T12:49:46.696300Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import requests"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Sequential\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LSTM, Dense, Dropout\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcallbacks\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m ModelCheckpoint\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'keras'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bc190c12"
   },
   "source": [
    "# ============================================\n",
    "# üìò CHAR-LEVEL LSTM TEXT GENERATOR\n",
    "# Trains a model to generate text similar to Shakespeare\n",
    "# ============================================\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import requests\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 1Ô∏è‚É£ LOAD DATA\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Download the Tiny Shakespeare dataset from GitHub\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "# Take only the first 100,000 characters for faster training\n",
    "text = requests.get(url).text[:100000]\n",
    "\n",
    "# Get all unique characters in the text (a‚Äìz, A‚ÄìZ, punctuation, etc.)\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "# Create dictionaries to convert characters ‚Üí numbers and numbers ‚Üí characters\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "\n",
    "# Show some info about the dataset\n",
    "print(f\"Unique characters: {len(chars)}\")\n",
    "print(f\"First 10 characters: {chars[:10]}\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 2Ô∏è‚É£ PREPARE TRAINING DATA\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Sequence length ‚Äî how many previous characters the model uses to predict the next one\n",
    "seq_length = 100\n",
    "\n",
    "# These lists will store input (dataX) and target output (dataY)\n",
    "dataX, dataY = [], []\n",
    "\n",
    "# Loop through the text and create sequences\n",
    "# Example: if seq_length=5, use chars [0..4] to predict char[5]\n",
    "for i in range(0, len(text) - seq_length):\n",
    "    seq_in = text[i:i + seq_length]         # 100-character input sequence\n",
    "    seq_out = text[i + seq_length]          # the next character after the sequence\n",
    "    dataX.append([char_to_int[char] for char in seq_in])  # encode input as integers\n",
    "    dataY.append(char_to_int[seq_out])                   # encode target char\n",
    "\n",
    "# Number of total patterns created\n",
    "n_patterns = len(dataX)\n",
    "# Number of unique characters (vocabulary size)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Patterns:\", n_patterns)\n",
    "\n",
    "# Reshape input data to be [samples, time steps, features]\n",
    "# Each input sequence has 100 time steps and 1 feature per step\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# Normalize data to 0‚Äì1 range (helps training)\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "# Convert output to one-hot encoded vectors\n",
    "# (for classification among all characters)\n",
    "Y = np.eye(n_vocab)[dataY]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 3Ô∏è‚É£ BUILD THE MODEL\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Sequential model = stack of layers\n",
    "model = Sequential()\n",
    "\n",
    "# First LSTM layer with 256 memory units, returning full sequence\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "# Dropout layer helps prevent overfitting (randomly disables 20% of neurons)\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second LSTM layer (final recurrent layer)\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Output layer ‚Äî predicts probability for each character\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "# Compile model using categorical crossentropy (for multi-class classification)\n",
    "# and Adam optimizer for efficient gradient updates\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 4Ô∏è‚É£ SAVE BEST MODEL DURING TRAINING\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Save model weights only when loss improves\n",
    "checkpoint = ModelCheckpoint(\n",
    "    \"text_gen_model.h5\",     # file to save the model\n",
    "    monitor='loss',          # track training loss\n",
    "    verbose=1,               # print updates\n",
    "    save_best_only=True,     # save only if loss decreases\n",
    "    mode='min'               # lower loss = better\n",
    ")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 5Ô∏è‚É£ TRAIN THE MODEL\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Train model for 20 epochs, batch size of 128\n",
    "# You can increase epochs for better results\n",
    "model.fit(X, Y, epochs=20, batch_size=128, callbacks=[checkpoint])\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 6Ô∏è‚É£ GENERATE TEXT\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "# Pick a random starting point (seed sequence)\n",
    "start = np.random.randint(0, len(dataX) - 1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "# Print the seed text (so we can see where generation starts)\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 7Ô∏è‚É£ TEXT GENERATION LOOP\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "generated_text = \"\"  # store generated output\n",
    "\n",
    "# Generate 1000 characters, one by one\n",
    "for i in range(1000):\n",
    "    # Reshape the pattern to match LSTM input\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    # Normalize input same as training\n",
    "    x = x / float(n_vocab)\n",
    "\n",
    "    # Predict next character probabilities\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    # Pick the most probable character (argmax)\n",
    "    index = np.argmax(prediction)\n",
    "    # Convert predicted index back to character\n",
    "    result = int_to_char[index]\n",
    "    # Append to generated text\n",
    "    generated_text += result\n",
    "\n",
    "    # Update input pattern ‚Äî add new char, remove first one\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "# ----------------------------------------------------------\n",
    "# 8Ô∏è‚É£ PRINT THE RESULT\n",
    "# ----------------------------------------------------------\n",
    "\n",
    "print(\"\\nGenerated text:\\n\")\n",
    "print(generated_text)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Model o‚Äòqitilgandan keyin, faqat generatsiya qismini qayta ishga tushiring\n",
    "\n",
    "generated_text = \"\"\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    generated_text += result\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print(\"\\nYaratilgan matn:\\n\")\n",
    "print(generated_text)\n"
   ],
   "metadata": {
    "id": "Pnm_u315-JYu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "\n",
    "\n",
    "text = requests.get(url).text[:10000]\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "\n",
    "char_to_int = {c:i for i, c in enumerate(chars)}\n",
    "int_to_char = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "\n",
    "print(f\"Unique characters: {len(chars)}\")\n",
    "print(f\"First 10 characters: {chars[:10]}\")\n",
    "\n",
    "seq_length = 100 # Corrected typo\n",
    "dataX, dataY = [], []\n",
    "\n",
    "\n",
    "for i in range(0, len(text) - seq_length): # Corrected typo\n",
    "    seq_in = text[i:i + seq_length] # Corrected typo\n",
    "    seq_out = text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Patterns:\", n_patterns)\n",
    "\n",
    "\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1)) # Corrected variable name\n",
    "X = X / float(n_vocab) # Added closing parenthesis"
   ],
   "metadata": {
    "id": "QNkYYXYh_Buy"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(x.shape[1], x.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(n_vocab, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "metadata": {
    "id": "ZOFeNPEF_1y6"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "O8NiciCr-7ur"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
